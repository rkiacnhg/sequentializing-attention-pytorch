{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sequentializing_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sequentializing Pytorch's Parallel Implementation of Attention and Analyzing the Difference in Performance Between the Two Implementations"
      ],
      "metadata": {
        "id": "qALrpjwpUwya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, List, Optional, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "from torch.nn.init import xavier_uniform_, constant_\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import gradcheck\n",
        "from torch.overrides import has_torch_function\n",
        "from torch.nn.functional import linear, softmax, dropout\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import math, time, copy\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, Transformer\n",
        "from torch.utils.data import dataset\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from prettytable import PrettyTable\n"
      ],
      "metadata": {
        "id": "_qypN_nBuP2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter):#: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data, bsz):#: Tensor, bsz: int) -> Tensor:\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)\n",
        "print(len(train_data)//35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9We0WK1vuSyl",
        "outputId": "c7ad6a0a-5081-4ae4-b6b6-0203dee97cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.48M/4.48M [00:00<00:00, 8.77MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "bptt = 35\n",
        "def get_batch(source, i ):# : Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "Eiak8ewOwus0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resource: https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
        "class sequentialMHA(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, query, key, value, w, b, head_dim):\n",
        "    ctx.save_for_backward(query,key,value,w,b,torch.Tensor([head_dim]))\n",
        "    Q = torch.matmul(query,w[:head_dim,:].T)\n",
        "    K = torch.matmul(key,w[head_dim:head_dim*2,:].T)\n",
        "    V = torch.matmul(value,w[head_dim*2:,:].T)\n",
        "    if b is not None:\n",
        "        Q += b[:head_dim]\n",
        "        K += b[head_dim:head_dim*2]\n",
        "        V += b[head_dim*2:]\n",
        "    return Q, K, V\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output_Q, grad_output_K, grad_output_V):\n",
        "    #print(grad_output_Q.shape)\n",
        "    q,k,v,w,b,head_dim = ctx.saved_tensors\n",
        "    grad_q = grad_k = grad_v = grad_w = grad_bias = None\n",
        "    head_dim = int(head_dim.item())\n",
        "    grad_q = torch.matmul(grad_output_Q, w[:head_dim,:])\n",
        "    grad_k = torch.matmul(grad_output_K, w[head_dim: head_dim*2,:])\n",
        "    grad_v = torch.matmul(grad_output_V, w[head_dim*2:,:])\n",
        "\n",
        "    grad_wq = torch.bmm(grad_output_Q.transpose(1,2), q)\n",
        "    grad_wk = torch.bmm(grad_output_K.transpose(1,2), k)\n",
        "    grad_wv = torch.bmm(grad_output_V.transpose(1,2), v)\n",
        "    grad_w = torch.cat((grad_wq, grad_wk, grad_wv), dim = 1)\n",
        "    if b is not None:\n",
        "      grad_bq = grad_output_Q.sum(0)\n",
        "      grad_bk = grad_output_K.sum(0)\n",
        "      grad_bv = grad_output_V.sum(0)\n",
        "      grad_bias = torch.cat((grad_bq, grad_bk, grad_bv), dim = 1)\n",
        "    return grad_q, grad_k, grad_v, grad_w, grad_bias, None\n",
        "    "
      ],
      "metadata": {
        "id": "gD2oUsN6S3_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ_9VpRSH8do"
      },
      "outputs": [],
      "source": [
        "#adapted from: https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py\n",
        "Tensor = torch.Tensor\n",
        "\n",
        "class CustomMultiheadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    \n",
        "    Modified pytorch official implementation to add memory efficient MHA: \n",
        "        pytorch repo :https://github.com/pytorch/pytorch\n",
        "\n",
        "    Allows the model to jointly attend to information\n",
        "    from different representation subspaces.\n",
        "    See reference: Attention Is All You Need\n",
        "\n",
        "    .. math::\n",
        "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
        "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
        "\n",
        "    Args:\n",
        "        embed_dim: total dimension of the model.\n",
        "        num_heads: parallel attention heads.\n",
        "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
        "        bias: add bias as module parameter. Default: True.\n",
        "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
        "        add_zero_attn: add a new batch of zeros to the key and\n",
        "                       value sequences at dim=1.\n",
        "        kdim: total number of features in key. Default: None.\n",
        "        vdim: total number of features in value. Default: None.\n",
        "\n",
        "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
        "        query, key, and value have the same number of features.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> multihead_attn = nn.CustomMultiheadAttention(embed_dim, num_heads)\n",
        "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "    \"\"\"\n",
        "    bias_k: Optional[torch.Tensor]\n",
        "    bias_v: Optional[torch.Tensor]\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, training=True, dropout=0., bias=True, \n",
        "        add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first = False,\n",
        "        memory_efficient=False):\n",
        "        super(CustomMultiheadAttention, self).__init__()\n",
        "        self.memory_efficient = memory_efficient\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "        self.training = training\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.batch_first = batch_first\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        if self._qkv_same_embed_dim is False:\n",
        "            self.q_proj_weight = nn.Parameter(torch.Tensor(embed_dim, embed_dim))\n",
        "            self.k_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.kdim))\n",
        "            self.v_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.vdim))\n",
        "            self.register_parameter('in_proj_weight', None)\n",
        "        else:\n",
        "            if memory_efficient:\n",
        "                self.in_proj_weight = [nn.Parameter(torch.empty(3 * self.head_dim, self.head_dim)) for ii in range(num_heads)]\n",
        "                self.in_proj_weight = nn.ParameterList(self.in_proj_weight)\n",
        "                self.mha = sequentialMHA.apply\n",
        "            else:\n",
        "                self.in_proj_weight = nn.Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
        "            self.register_parameter('q_proj_weight', None)\n",
        "            self.register_parameter('k_proj_weight', None)\n",
        "            self.register_parameter('v_proj_weight', None)\n",
        "\n",
        "        if bias:\n",
        "            if memory_efficient:\n",
        "                self.in_proj_bias = [nn.Parameter(torch.empty(3 * self.head_dim)) for ii in range(num_heads)]\n",
        "                self.in_proj_bias = nn.ParameterList(self.in_proj_bias)\n",
        "            else:\n",
        "                self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim)) \n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "        self.out_proj =  nn.Linear(embed_dim, embed_dim, bias = True)\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = nn.Parameter(torch.empty(1, 1, embed_dim))\n",
        "            self.bias_v = nn.Parameter(torch.empty(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        if self._qkv_same_embed_dim:\n",
        "            if self.memory_efficient:\n",
        "                for ii in range(len(self.in_proj_weight)):\n",
        "                    nn.init.xavier_uniform_(self.in_proj_weight[ii])\n",
        "            else:\n",
        "                nn.init.xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.q_proj_weight)\n",
        "            nn.init.xavier_uniform_(self.k_proj_weight)\n",
        "            nn.init.xavier_uniform_(self.v_proj_weight)\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            if self.memory_efficient:\n",
        "                for ii in range(len(self.in_proj_bias)):\n",
        "                    nn.init.constant_(self.in_proj_bias[ii], 0.)\n",
        "            else:\n",
        "                nn.init.constant_(self.in_proj_bias, 0.)\n",
        "            nn.init.constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            nn.init.xavier_normal_(self.bias_v)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        # Support loading old CustomMultiheadAttention checkpoints generated by v1.1.0\n",
        "        if '_qkv_same_embed_dim' not in state:\n",
        "            state['_qkv_same_embed_dim'] = True\n",
        "\n",
        "        super(CustomMultiheadAttention, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None,\n",
        "                need_weights=True, attn_mask=None):\n",
        "        \"\"\"\n",
        "    Args:\n",
        "        query, key, value: map a query and a set of key-value pairs to an output.\n",
        "            See \"Attention Is All You Need\" for more details.\n",
        "        key_padding_mask: if provided, specified padding elements in the key will\n",
        "            be ignored by the attention. When given a binary mask and a value is True,\n",
        "            the corresponding value on the attention layer will be ignored. When given\n",
        "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
        "            layer will be ignored\n",
        "        need_weights: output attn_output_weights.\n",
        "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
        "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
        "\n",
        "    Shape:\n",
        "        - Inputs:\n",
        "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
        "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
        "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
        "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
        "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
        "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
        "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
        "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
        "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
        "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
        "          is provided, it will be added to the attention weight.\n",
        "\n",
        "        - Outputs:\n",
        "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
        "          E is the embedding dimension.\n",
        "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
        "          L is the target sequence length, S is the source sequence length.\n",
        "        \"\"\"\n",
        "        if self.memory_efficient:\n",
        "            #code adapted from: https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py\n",
        "\n",
        "            if self.batch_first:\n",
        "              key = key.transpose(0,1)\n",
        "              query = query.transpose(0,1)\n",
        "              value = value.transpose(0,1)\n",
        "            tgt_len, bsz, embed_dim = query.shape\n",
        "            src_len, _, _ = key.shape\n",
        "\n",
        "            #Check if batched. (Is this necessary, or can we assume the shape indicated in the documentation?)\n",
        "            if query.dim() == 3:\n",
        "                # Batched Inputs\n",
        "                is_batched = True\n",
        "                assert key.dim() == 3 and value.dim() == 3, \\\n",
        "                    (\"For batched (3-D) `query`, expected `key` and `value` to be 3-D\"\n",
        "                    f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\")\n",
        "                if key_padding_mask is not None:\n",
        "                    assert key_padding_mask.dim() == 2, \\\n",
        "                        (\"For batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\"\n",
        "                        f\" but found {key_padding_mask.dim()}-D tensor instead\")\n",
        "                if attn_mask is not None:\n",
        "                    assert attn_mask.dim() in (2, 3), \\\n",
        "                        (\"For batched (3-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
        "                        f\" but found {attn_mask.dim()}-D tensor instead\")\n",
        "            elif query.dim() == 2:\n",
        "                # Unbatched Inputs\n",
        "                is_batched = False\n",
        "                assert key.dim() == 2 and value.dim() == 2, \\\n",
        "                    (\"For unbatched (2-D) `query`, expected `key` and `value` to be 2-D\"\n",
        "                    f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\")\n",
        "\n",
        "                if key_padding_mask is not None:\n",
        "                    assert key_padding_mask.dim() == 1, \\\n",
        "                        (\"For unbatched (2-D) `query`, expected `key_padding_mask` to be `None` or 1-D\"\n",
        "                        f\" but found {key_padding_mask.dim()}-D tensor instead\")\n",
        "\n",
        "                if attn_mask is not None:\n",
        "                    assert attn_mask.dim() in (2, 3), \\\n",
        "                        (\"For unbatched (2-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
        "                        f\" but found {attn_mask.dim()}-D tensor instead\")\n",
        "                    if attn_mask.dim() == 3:\n",
        "                        expected_shape = (num_heads, query.shape[0], key.shape[0])\n",
        "                        assert attn_mask.shape == expected_shape, \\\n",
        "                            (f\"Expected `attn_mask` shape to be {expected_shape} but got {attn_mask.shape}\")\n",
        "            else:\n",
        "                raise AssertionError(\n",
        "                    f\"query should be unbatched 2D or batched 3D tensor but received {query.dim()}-D query tensor\")\n",
        "            \n",
        "            #Make into batch size 1 if not batched\n",
        "            if not is_batched:\n",
        "                # unsqueeze if the input is unbatched\n",
        "                query = query.unsqueeze(1)\n",
        "                key = key.unsqueeze(1)\n",
        "                value = value.unsqueeze(1)\n",
        "                if key_padding_mask is not None:\n",
        "                    key_padding_mask = key_padding_mask.unsqueeze(0)\n",
        "            \n",
        "            #Check attn mask\n",
        "            if attn_mask is not None:\n",
        "                if attn_mask.dtype == torch.uint8:\n",
        "                    warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
        "                    attn_mask = attn_mask.to(torch.bool)\n",
        "                else:\n",
        "                    assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
        "                        f\"Only float, byte, and bool types are supported for attn_mask, not {attn_mask.dtype}\"\n",
        "                # ensure attn_mask's dim is 3\n",
        "                if attn_mask.dim() == 2:\n",
        "                    correct_2d_size = (tgt_len, src_len)\n",
        "                    if attn_mask.shape != correct_2d_size:\n",
        "                        raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
        "                    attn_mask = attn_mask.unsqueeze(0)\n",
        "                elif attn_mask.dim() == 3:\n",
        "                    correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
        "                    if attn_mask.shape != correct_3d_size:\n",
        "                        raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
        "                else:\n",
        "                    raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
        "                if(self.bias_k is not None and self.bias_v is not None):\n",
        "                    attn_mask = F.pad(attn_mask, (0,1))\n",
        "                if(self.add_zero_attn):\n",
        "                    attn_mask = F.pad(attn_mask, (0,1))\n",
        "\n",
        "            # Add dim according to the number of paddings done to masks\n",
        "            if self.bias_k is not None and self.bias_v is not None:\n",
        "                src_len +=1\n",
        "            if self.add_zero_attn:\n",
        "                src_len +=1\n",
        "            \n",
        "            #Check key padding mask\n",
        "            if key_padding_mask is not None:\n",
        "                if key_padding_mask.dtype == torch.uint8:\n",
        "                    key_padding_mask = key_padding_mask.to(torch.bool)\n",
        "                if(self.add_zero_attn):\n",
        "                    key_padding_mask = F.pad(key_padding_mask, (0,1))\n",
        "                if(self.bias_k is not None and self.bias_v is not None):\n",
        "                    key_padding_mask = F.pad(key_padding_mask, (0,1))\n",
        "\n",
        "                assert key_padding_mask.shape == (bsz, src_len), \\\n",
        "                    f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
        "                key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).   \\\n",
        "                    expand(-1, self.num_heads, -1, -1).reshape(bsz * self.num_heads, 1, src_len)\n",
        "                if attn_mask is None:\n",
        "                    attn_mask = key_padding_mask\n",
        "                elif attn_mask.dtype == torch.bool:\n",
        "                    attn_mask = attn_mask.logical_or(key_padding_mask)\n",
        "                else:\n",
        "                    attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n",
        "\n",
        "            #Finalize mask\n",
        "            if attn_mask is not None and attn_mask.dtype == torch.bool:\n",
        "                new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n",
        "                new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n",
        "                attn_mask = new_attn_mask\n",
        "\n",
        "            #Reshape q,k,v for sequential operations (Seq_len, Batch_size, Num_heads, Head_dim)\n",
        "            query = query.view(query.shape[0],query.shape[1],self.num_heads,self.head_dim)\n",
        "            key = key.view(key.shape[0],key.shape[1],self.num_heads,self.head_dim)\n",
        "            value = value.view(value.shape[0],value.shape[1],self.num_heads,self.head_dim)\n",
        "\n",
        "            for i in range(self.num_heads):\n",
        "                Q, K, V = self.mha(query[:,:,i,:], key[:,:,i,:], value[:,:,i,:], self.in_proj_weight[i], self.in_proj_bias[i], self.head_dim)\n",
        "                if self.bias_k is not None and self.bias_v is not None:\n",
        "                    K = torch.cat([K, self.bias_k.repeat(1, bsz, 1)[:,:,self.head_dim*i: self.head_dim*(i+1)]])\n",
        "                    V = torch.cat([V, self.bias_v.repeat(1, bsz, 1)[:,:,self.head_dim*i: self.head_dim*(i+1)]])\n",
        "                \n",
        "                #make batch first\n",
        "                Q = Q.transpose(0,1)\n",
        "                K = K.transpose(0,1)\n",
        "                V = V.transpose(0,1)\n",
        "\n",
        "                if self.add_zero_attn:\n",
        "                    zero_attn_shape = (bsz, 1, self.head_dim)\n",
        "                    K = torch.cat([K, torch.zeros(zero_attn_shape, dtype=K.dtype, device=K.device)], dim=1)\n",
        "                    V = torch.cat([V, torch.zeros(zero_attn_shape, dtype=V.dtype, device=V.device)], dim=1)\n",
        "                \n",
        "                # scaled dot product attention\n",
        "                Q /= math.sqrt(self.head_dim)\n",
        "                qk = torch.matmul(Q, K.transpose(-1,1))\n",
        "                \n",
        "                if(attn_mask is not None):\n",
        "                    qk = qk + attn_mask[bsz*i:bsz*(i+1),:,:] if attn_mask.shape[0] > 1 else qk + attn_mask\n",
        "                \n",
        "                attn = nn.Softmax(dim=-1)(qk)\n",
        "                if(self.training and self.dropout > 0.0):\n",
        "                    attn = nn.Dropout(p = self.dropout)(attn)\n",
        "                \n",
        "                # Concatenate weights and outputs for final out_projection after iteration\n",
        "                attn_weights = attn if i == 0 else torch.cat((attn_weights, attn))\n",
        "                attn_output = torch.matmul(attn, V) if i == 0 else torch.cat((attn_output,torch.matmul(attn,V)), dim = -1)\n",
        "            \n",
        "            attn_output = attn_output.transpose(0,1)\n",
        "            out = self.out_proj(attn_output)\n",
        "            if need_weights:\n",
        "                attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "                attn_weights = attn_weights.sum(dim = 1) / self.num_heads\n",
        "                if not is_batched:\n",
        "                    attn_output = attn_output.squeeze(1)\n",
        "                    attn_weights = attn_weights.squeeze(0)\n",
        "                return attn_output, attn_weights\n",
        "            else:\n",
        "                if not is_batched:\n",
        "                    attn_output = attn_output.squeeze(1)\n",
        "                return attn_output, None\n",
        "        else:\n",
        "            if not self._qkv_same_embed_dim:\n",
        "                return F.multi_head_attention_forward(\n",
        "                    query, key, value, self.embed_dim, self.num_heads,\n",
        "                    self.in_proj_weight, self.in_proj_bias,\n",
        "                    self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                    self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                    training=self.training,\n",
        "                    key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                    attn_mask=attn_mask, use_separate_proj_weight=True,\n",
        "                    q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
        "                    v_proj_weight=self.v_proj_weight)\n",
        "            else:\n",
        "                #\n",
        "                return F.multi_head_attention_forward(\n",
        "                    query, key, value, self.embed_dim, self.num_heads,\n",
        "                    self.in_proj_weight, self.in_proj_bias,\n",
        "                    self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                    self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                    training=self.training,\n",
        "                    key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                    attn_mask=attn_mask)\n",
        "\n",
        "\n",
        "# Generate mask covering the top right triangle of a matrix\n",
        "def generate_square_subsequent_mask(size):\n",
        "\tmask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "\tmask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\treturn mask\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):#: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "mniD9tAmx6re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTransformerEncoderLayer(nn.modules.TransformerEncoderLayer):\n",
        "  def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "              activation = F.relu,\n",
        "              layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
        "              device=None, dtype=None) -> None:\n",
        "      super(CustomTransformerEncoderLayer, self).__init__(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, device, dtype)\n",
        "      self.self_attn = CustomMultiheadAttention(d_model, nhead, dropout = dropout, memory_efficient= True)#, add_bias_kv = True, add_zero_attn = True)"
      ],
      "metadata": {
        "id": "5rzbgWbZIhjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):# -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):#: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "class CustomTransformerModel(TransformerModel):\n",
        "  def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super(CustomTransformerModel, self).__init__(ntoken, d_model, nhead, d_hid, nlayers, dropout)\n",
        "        encoder_layers = CustomTransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "\n"
      ],
      "metadata": {
        "id": "fg2CMZbfj9Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modified code from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "def train(model, optimizer, criterion, train_data, scheduler) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    ntokens = len(vocab)\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in tqdm(enumerate(range(0, train_data.size(0) - 1, bptt))):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        batch_size = data.size(0)\n",
        "        if batch_size != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data,  src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            #print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "            #      f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "            #      f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        #if batch > 200:\n",
        "        #  break\n",
        "        del data\n",
        "        del targets\n",
        "        del output\n",
        "        del loss\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "\n",
        "def evaluate(model, criterion, eval_data):\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(vocab)\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            batch_size = data.size(0)\n",
        "            if batch_size != bptt:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "xYb_O4DWyIKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#source: https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\n",
        "#model parameter count measurement\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "#count_parameters(model)"
      ],
      "metadata": {
        "id": "eA699huThKnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# time/memory/performance evaluation\n",
        "\n",
        "import time\n",
        "def run():\n",
        "    typ = \"sequential\"\n",
        "    nhead = 4\n",
        "    emsize = 200\n",
        "    nlayers = 3\n",
        "    lr = 5.0  # learning rate\n",
        "    epochs = 3\n",
        "    ntokens = len(vocab)  # size of vocabulary\n",
        "    d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "    dropout = 0.2  # dropout probability\n",
        "    if typ == \"sequential\":\n",
        "      model = CustomTransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "    else:\n",
        "      model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()p\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)    \n",
        "    for epoch in range(epochs):  \n",
        "        epoch_start_time = time.time()\n",
        "        train(model, optimizer, criterion, train_data, scheduler)\n",
        "        elapsed = time.time() - epoch_start_time\n",
        "        #print(\"time:\", elapsed)\n",
        "        #break\n",
        "        val_loss = evaluate(model, criterion, val_data)\n",
        "        val_ppl = math.exp(val_loss)\n",
        "        scheduler.step()\n",
        "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "              f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "        #break\n",
        "\n",
        "    #source for model size computation: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
        "    mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])\n",
        "    mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])\n",
        "    mem = mem_params + mem_bufs # in bytes\n",
        "    \n",
        "    #print(typ, nlayers, emsize, nhead, batch_size)\n",
        "    #print(torch.cuda.max_memory_allocated(), torch.cuda.max_memory_reserved(), mem)\n",
        "    print(\"--\"*36)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "#Uncomment to run\n",
        "#run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvz9FFyS1XCR",
        "outputId": "2eb1f62a-07bf-4575-af77-a8dc45445ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2929it [01:38, 29.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| end of epoch   0 | time: 98.77s | valid loss  5.79 | valid ppl   328.57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2929it [01:38, 29.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| end of epoch   1 | time: 98.42s | valid loss  5.58 | valid ppl   264.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2929it [01:38, 29.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| end of epoch   2 | time: 98.49s | valid loss  5.54 | valid ppl   255.19\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing MHA implementation\n",
        "if __name__ == \"__main__\":\n",
        "    src_seq_len = 2; tgt_seq_len = 2; b=7; emb_dim = 20; nhead = 4\n",
        "    torch.manual_seed(0)\n",
        "    Q = torch.rand((src_seq_len, b, emb_dim), requires_grad=True)\n",
        "    K = torch.rand((src_seq_len, b, emb_dim), requires_grad=True)\n",
        "    V = torch.rand((src_seq_len, b, emb_dim), requires_grad=True) #(2,7,20)\n",
        "    key_padding_mask =  torch.ones(b, src_seq_len)                #(7,2)\n",
        "    #key_padding_mask = None\n",
        "    attn_mask = generate_square_subsequent_mask(size=Q.shape[0])  #(2,2)\n",
        "    \n",
        "    MHA = CustomMultiheadAttention(emb_dim, nhead, training=True, memory_efficient=False, add_bias_kv= True, add_zero_attn = True)\n",
        "    out, w  = MHA(Q,Q,Q, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "    print(out[0,0])\n",
        "    \n",
        "    # your task is to implement `CustomMultiheadAttention` with memory_efficient=True and use it for the end task\n",
        "    MHA = CustomMultiheadAttention(emb_dim, nhead, training=True, memory_efficient=True, add_bias_kv = True, add_zero_attn = True)\n",
        "    out, w  = MHA(Q,Q,Q, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "    print(out[0,0])\n",
        "    loss = torch.sum(out)\n",
        "    loss.backward()"
      ],
      "metadata": {
        "id": "qHOI_XXiOgBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graphing\n",
        "#Model architecture: layers=13,  embed_dim=5000, hidden_dim=200\n",
        "\n",
        "gigabyte = 1000000000\n",
        "megabyte = 1000000\n",
        "#parallel\n",
        "x = [2, 4, 8] #heads\n",
        "parallel_time = [3227,3087.8,3234.2]\n",
        "parallel_maxbatch = [20,20,20]\n",
        "parallel_memory = [15753884672/gigabyte  ,15753884672/gigabyte ,15778738176/gigabyte]\n",
        "parallel_model_size = [6557745528/megabyte,6557745528/megabyte,6557745528/megabyte]\n",
        "\n",
        "\n",
        "#sequential\n",
        "seq_time = [3924.86, 1625.5, 1388.4]\n",
        "seq_maxbatch = [20, 90, 110]\n",
        "seq_memory = [14613331456/gigabyte, 8747521536/gigabyte, 7175637504/gigabyte]\n",
        "seq_model_size = [4607745528/megabyte, 3632745528/megabyte, 3145245528/megabyte]"
      ],
      "metadata": {
        "id": "dwpQOk77-ohY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = {\"Parallel\": parallel_memory, \"Sequential\": seq_memory}\n",
        "mem_df = pd.DataFrame(data = memory, index = [2,4,8])\n",
        "mem_df.plot(kind = \"bar\")\n",
        "plt.title(\"A: Peak GPU Usage in Gigabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "QbsQjSmcM0Lg",
        "outputId": "840437ba-158c-4c51-cc67-a2fad1deb8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'A: Peak GPU Usage in Gigabytes')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = {\"Parallel\": parallel_maxbatch, \"Sequential\": seq_maxbatch}\n",
        "batch_df = pd.DataFrame(data = batch, index = [2,4,8])\n",
        "batch_df.plot(kind = \"bar\")\n",
        "plt.title(\"B: Maximum Batch Size\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ZAjDAmBrM42M",
        "outputId": "aa7f208f-8310-4411-93c2-6cd2f73eb9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'B: Maximum Batch Size')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time = {\"Parallel\": parallel_time, \"Sequential\": seq_time}\n",
        "time_df = pd.DataFrame(data = time, index = [2,4,8])\n",
        "time_df.plot(kind = \"bar\")\n",
        "plt.title(\"C: Time Taken in Seconds for Max Possible Batch Size\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "WSx9XSjLQWpc",
        "outputId": "8dd999f6-9d7f-4c4a-d741-e430f80ce43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'C: Time Taken in Seconds for Max Possible Batch Size')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = {\"Parallel\": parallel_model_size, \"Sequential\": seq_model_size}\n",
        "size_df = pd.DataFrame(data = model_size, index = [2,4,8])\n",
        "size_df.plot(kind = \"bar\")\n",
        "plt.title(\"D: Model Sizes in Megabytes\")\n"
      ],
      "metadata": {
        "id": "2iUYaRzdQwnf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b1d55cd8-d367-41e4-fe9f-dfa94067dc20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'D: Model Sizes in Megabytes')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approach\n",
        "\n",
        "***Sequential Implementation:*** \n",
        "*self.in_proj_weight =  [nn.Parameter(torch.empty(3 x self.head_dim, self.head_dim)) for ii in range(num_heads)]*\n",
        "\n",
        "***Parallel Implementation***: *self.in_proj_weight = nn.Parameter(torch.empty(3 * embed_dim, embed_dim))*\n",
        "\n",
        "Given the definition of *self.in_proj_weight =  [nn.Parameter(torch.empty(3 x self.head_dim, self.head_dim)) for ii in range(num_heads)]*, I reshaped the query, key and value tensors to *(Seq_len, Batch_size, Num_heads, Head_dim)* and iterated over *num_heads* to perform matrix multiplication between *in_proj_weight[i] and query [: , : , i , : ] , k[ : , : , i , : ], v[ : , : , i , : ]* tensors (and similarly, matrix addition with *in_proj_bias*). \n",
        "\n",
        "Consequently, compared to the original, parallel implementation of MultiHeadAttention from Pytorch, the sequential implementation needed to take care of the *add_bias_kv* and *add_zero_attn* conditions inside the for loop, and needed to make changes to the *key_padding_mask* and *attn_mask* before the for loop. The shapes of the masks are computed and checked before the for loop, and Key and Value, if *add_bias_kv = True* and/or *add_zero_attn = True*, are handled within the for loop. \n",
        "\n",
        "# Performance:\n",
        "\n",
        "### Parallel num_heads = 2\n",
        "epoch   0 | time: 70.76s | valid loss  5.81 | valid ppl   334.35\n",
        "\n",
        "epoch   1 | time: 70.59s | valid loss  5.66 | valid ppl   286.65\n",
        "\n",
        "epoch   2 | time: 70.48s | valid loss  5.64 | valid ppl   280.98\n",
        "\n",
        "### Sequential num_heads = 2\n",
        "epoch   0 | time: 76.88s | valid loss  5.78 | valid ppl   323.44\n",
        "\n",
        "epoch   1 | time: 76.75s | valid loss  5.64 | valid ppl   280.60\n",
        "\n",
        "epoch   2 | time: 76.79s | valid loss  5.55 | valid ppl   258.29\n",
        "\n",
        "### Parallel num_heads = 4\n",
        "epoch   0 | time: 69.31s | valid loss  5.79 | valid ppl   327.99\n",
        "\n",
        "epoch   1 | time: 70.19s | valid loss  5.65 | valid ppl   283.42\n",
        "\n",
        "epoch   2 | time: 70.13s | valid loss  5.54 | valid ppl   255.39\n",
        "\n",
        "### Sequential num_heads = 4\n",
        "\n",
        "epoch   0 | time: 87.14s | valid loss  5.72 | valid ppl   305.81\n",
        "\n",
        "epoch   1 | time: 87.15s | valid loss  5.61 | valid ppl   273.97\n",
        "\n",
        "epoch   2 | time: 86.46s | valid loss  5.54 | valid ppl   253.51\n",
        "\n",
        "### Parallel num_heads = 8\n",
        "epoch   0 | time: 72.59s | valid loss  5.78 | valid ppl   325.22\n",
        "\n",
        "epoch   1 | time: 72.15s | valid loss  5.61 | valid ppl   273.99\n",
        "\n",
        "epoch   2 | time: 72.17s | valid loss  5.58 | valid ppl   264.19\n",
        "\n",
        "### Sequential num_heads = 8\n",
        "epoch   0 | time: 111.50s | valid loss  5.74 | valid ppl   311.85\n",
        "\n",
        "epoch   1 | time: 111.24s | valid loss  5.62 | valid ppl   276.88\n",
        "\n",
        "epoch   2 | time: 111.04s | valid loss  5.58 | valid ppl   265.14\n",
        "\n",
        "\n",
        "This log of the two implementations show that they perform almost equally well in terms of validation loss and validation ppl. In addition, the time taken for the Parallel implementation shows only a small difference in the time taken for different number of heads. On the other hand, sequential implementations show an increase in time taken as the number of heads grows. \n",
        "\n",
        "# Comparisons\n",
        "\n",
        "The comparison between Sequential and Parallel implementations was done with a fixed model architecture, with *layers = 13* and *embedding_size = 5000*. This architecture was the largest possible model size on my machine, for a Parallel implementation with *batch_size = 20*. \n",
        "The anticipated result of the comparison was that a Parallel implementation required more memory, but required less time to compute. Likewise, a Sequential implementation required less memory, but consumed more time to compute. \n",
        "\n",
        "I used the same model configurations for the Parallel and Sequential implementations, and found that the actual results slightly differed from this expectation.I've also included a graph comparing model sizes.\n",
        "\n",
        "### Peak GPU Usage\n",
        "To compare Peak GPU usage, I used *torch.cuda.max_memory_allocated()*. The Peak GPU usage graph shows that for Parallel implementation, the peak GPU usage is approximately the same, whereas the peak GPU usage for the Sequential implementation decreases as the number of heads increases, as desired. The primary reason for the decrease in GPU usage is that the number of parameters of MultiHead Attention for models with Parallel implementation do not depend on the number of heads, and only depend on the embedding size, as shown by the definition of weights: *self.in_proj_weight = Parameter(torch.empty((3 x embed_dim, embed_dim)))*. Compared to this, the number of parameters of attention for Sequential implementation depends on the number of heads: *self.in_proj_weight = [nn.Parameter(torch.empty(3 x self.head_dim, self.head_dim)) for ii in range(num_heads)]* where *self.head_dim = embed_dim // num_heads*. This dependence on *num_heads* allows models with sequential implementation to use much less memory for the model than Parallel models, by a factor of *num_heads*.\n",
        "\n",
        "### Maximum Batch Size\n",
        "Because the model size was much smaller for Sequential implementations, there was more memory available to be allocated to larger batches of data. As a result, the maximum batch size becomes larger as *num_heads* increases for Sequential implementations. However, because the model size was constant for Parallel implementation, the maximum batch size for Parallel implementation was the same.\n",
        "\n",
        "### Run-Time for Max Batch \n",
        "When *num_heads = 2*, we observe that the Sequential Implementation takes more time than the Parallel Implementation even though the Sequential Implementation's model size is smaller than the Parallel's. As *num_heads* increases, we notice that the time taken for Sequential implementation decreases, taking only half of the time of Parallel implementations. This can be attributed to the fact the Sequential implementations require less model memory, allowing the maximum possible batch size to be much larger than the maximum possible batch size for Parallel implementations. However, as the Performance section shows, under similar circumstances, the runtime of Parallel implementation easily outperforms the runtime of Sequential implementation.\n",
        "\n",
        "### Future work\n",
        "Given more time, it would be interesting to study some kind of a memory threshold for which Parallel implementations perform better than Sequential implementations. In the Performance section, it is evident that the Sequential implementation takes longer than the Parallel implementation, and the runtime increases as the number of heads increases. However, when experimenting with a model that was created to be as large as possible for *batch_size = 20* for the Parallel implementation, we discover that the Sequential models are much smaller and result in better runtimes. In fact, the Sequential models' performances can be further improved by increasing the batch size, up to 110, for *num_heads = 8*. We can understand that the time efficiency of Parallel implementations are not always better than Sequential implementations', and past a certain memory threshold, perform worse than Sequential implementations in terms of both memory and runtime. Investigating appropriate model sizes relative to available memory and identifying some threshold of model sizes at which Sequential implementations gain the upper hand for both memory and runtime could be valuable insight for deciding which implementation to use.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l_RewKc0aOqu"
      }
    }
  ]
}